Currently running:

NN eval (on Sherlock)
Sampling from trajectory
T = 500,000
3x longer to train "true" Q
lr = 0.1
batch size = 50
Results: Errors seem to have stabilized, although UB still
not quite as good as true. MC is also jagged and doesn't
quite match UB/true/BFF. Rerun with more reps for MC and
more iterations.


NN control (on laptop)
Uniform sampling
T = 100,000
3x longer to train "true" Q
lr = 0.1
batch size = 50
e = 0.1
Results: Error still decaying at termination.


Tabular eval (on laptop)
Uniform sampling
T = 1,000,000
lr = 0.1
batch size = 50
Result: Error still decaying at termination.
Also, unwittingly set sigma = 0 (no diffusion).
Re-run with T = 10^7, sigma = 1.


Tabular eval (Sherlock)
Uniform sampling
T = 10,000,000
lr = 0.01
bs = 50
Result: Error still decaying at termination.
Results look good, bff and ub are almost identical.
Run again for longer.
Don't need Monte Carlo since true is solved with Bellman
equation.


NOTE: FROM HERE ON OUT, save plots as <expt #>_<plt type>.png
e.g. 1_error.png, 1_bellman.png, 1_q.png

Tabular eval (Sherlock) #1
Sampling: Uniform
T = 10,000,000
lr = 0.5
bs = 50
Results: Looks great! BFF and UB both match true closely.
Re-ru
n with trajectory sampling (experiment #5).

NN eval (Sherlock) #2
Sampling: Trajectory
T = 1,000,000 (3x longer to train "true" Q)
lr = 0.1
bs = 50
Results: Success! NN eval is completed.


NN control (Sherlock) #3
Sampling: Uniform
T = 1,000,000 (3x longer to train "true" Q)
lr = 0.1
bs = 50
e = 0.1
Results: Results look good except for BFF.
After revisiting the code, this is probably because the action
was being updated incorrectly (at the very least, not uniform).
Fix the action update and re-run with same hyperparams.


NN control (Sherlock) #4
Sampling: Trajectory
T = 1,000,000 (3x longer to train "true" Q)
lr = 0.1
bs = 50
e = 0.1
Results: The Q function is accurate on high-value states,
but inaccurate for low value states. Increase e to 0.5 and re-run.


Tabular eval (Sherlock) #5
Sampling: Trajectory
T = 10,000,000
lr = 0.5
bs = 50
Results: Success! Tabular eval is completed.


NN control (Sherlock) #6
Sampling: Uniform
T = 1,000,000 (3x longer to train "true" Q)
lr = 0.1
bs = 50
e = 0.1
Fixed bug with BFF. (See experiment #3.)
Results: BFF still not converging. Looks like there may have also been a bug
with ftr_s. Also, for consistency with the other methods, nxt_a should be sampled
uniformly instead of greedily.
Also, for the MC estimate, the trajectory should be generated with full greedy
policy, not e-greedy. Fix and rerun with same hyperparams.


NN control (Sherlock) #7
Sampling: Trajectory
T = 1,000,000 (3x longer to train "true" Q)
lr = 0.1
bs = 50
e = 0.5
Results: Looks much better, getting closer to correct values on low-value states.
MC looks noticeably lower than other methods, could be due to incorrect action
choice (see note on experiment #6). Fix MC action selection, then re-run.
Also, generate some plots without MC.


NN control (Sherlock) #8
Sampling: Uniform
T = 1,000,000 (3x longer to train "true" Q)
lr = 0.1
bs = 50
e = 0.1
Fixed more bugs with BFF. Also changed policy used for MC. (See experiment #6.)
Results: Success!


NN control (Sherlock) #9
Sampling: Trajectory
T = 1,000,000 (3x longer to train "true" Q)
lr = 0.1
bs = 50
e = 0.5
Results: Success!


Tabular eval (Sherlock) #10
Sampling: Trajectory
T = 10,000,000
lr = 0.5
bs = 50
Changed method to bff-gd instead of bff-loss. Crucial algorithm change,
bff-loss doesn't work for Q evaluation (see paper).
Results: Performs even better than previous success (experiment #5). BFF is
almost indistinguishable from UB.